seed: 0
device: auto
out_dir: runs/default

env:
  n_defenders: 5
  n_attackers: 5
  dt: 0.1
  max_steps: 600

  # 命中半径（D 固定截击距离，小于 P 的打击半径）
  t_attack_radius: 9.0
  d_attack_radius: 6.5
  t_threat_radius: 24.0
  d_threat_radius: 6.5

  # 目标初速度（T）
  t_velocity: [10.0, 0.0, 0.0]

  # 攻击者(P)/防守者(D)的速度/加速度上限（确保 D 略弱于 P）
  p_speed_max: 23.5
  p_accel_max: 5.2
  d_speed_max: 18.5
  d_accel_max: 3.2

  # 世界边界 [xmin,xmax], [ymin,ymax], [zmin,zmax]
  world_bounds: [[-1000.0, 1000.0], [-300.0, 300.0], [-150.0, 150.0]]

  # 开局生成参数（给予更长前出距离，便于弱机动 D 提前截击）
  spawn:
    t_x_frac: 0.5
    d_guard_radius: 36.0
    front_hit_time: 36.0
    lateral_spread_y: 320.0
    lateral_spread_z: 220.0
    min_pp_dist: 240.0
    min_tp_dist: 140.0
    spawn_attempts: 200

  # 奖励塑形（鼓励开局即主动截击并保持效率）
  approach_reward_scale: 0.003
  closing_reward_scale: 0.003
  kill_reward: 1.3
  time_penalty: 0.0012
  failure_penalty: 2.2
  success_bonus: 2.0
  defender_loss_penalty: 0.25

control:
  # 基线控制：使用主动截击模型，并保持部分余量给残差策略
  base_type: attack
  base_kp: 0.18
  base_kd: 0.55
  attack_kp: 0.4
  attack_kd: 0.75
  attack_lead_time: 3.2
  attack_bias: 0.65
  base_alpha: 0.85
  residual_gain: 1.0
  pn_nav_gain: 3.8
  manager_period: 1
  imitation_min_budget: 0.02

matcher:
  algo: hungarian
  assign_lock_steps: 8
  switch_penalty: 12.0

train:
  updates: 400
  horizon: 600
  gamma: 0.99
  gae_lambda: 0.95
  lr: 0.0003
  clip_eps: 0.2
  epochs: 4
  batch_size: 1024
  value_coef: 0.5
  entropy_coef: 0.01

  # 残差模仿权重线性衰减
  imitation_w_start: 1.0
  imitation_w_end: 0.2
  imitation_decay_updates: 400
