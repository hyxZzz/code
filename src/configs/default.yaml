seed: 0
device: auto
out_dir: runs/default

env:
  n_defenders: 5
  n_attackers: 5
  dt: 0.1
  max_steps: 600

  # 命中半径
  t_attack_radius: 8.0
  d_attack_radius: 24.0
  t_threat_radius: 60.0      # ↑ 威胁判定提前，留出机动缓冲区（仅放宽命中判定，仍会提前前出）
  d_threat_radius: 45.0      # ↑ 威胁态下允许更远距离拦截

  # 目标初速度（T）
  t_velocity: [10.0, 0.0, 0.0]

  # 攻击者(P)/防守者(D)的速度/加速度上限
  p_speed_max: 24.0
  p_accel_max: 6.0
  d_speed_max: 20.0
  d_accel_max: 4.0

  # 世界边界 [xmin,xmax], [ymin,ymax], [zmin,zmax]
  world_bounds: [[-1000.0, 1000.0], [-300.0, 300.0], [-150.0, 150.0]]

  # 开局生成参数
  spawn:
    t_x_frac: 0.5
    d_guard_radius: 40.0
    front_hit_time: 30.0
    lateral_spread_y: 400.0
    lateral_spread_z: 300.0
    min_pp_dist: 300.0
    min_tp_dist: 120.0
    spawn_attempts: 200

  # 奖励塑形（让 D 更积极接战）
  approach_reward_scale: 0.004   # ↓ 让塑形不主导优化
  closing_reward_scale: 0.003    # ↓ 稍弱的闭合奖励
  kill_reward: 1.2               # ↑ 适度奖励击杀
  time_penalty: 0.0008           # ↑ 失败拖延的惩罚
  failure_penalty: 2.0           # 新增：目标被击中直接-2
  success_bonus: 2.0             # ↑ 成功护航奖励
  defender_loss_penalty: 0.2     # 新增：防守方损失惩罚

control:
  # 基线控制策略（更偏向主动拦截）
  base_type: attack             # escort -> attack

  # 护航/基础控制的 PD（保留以便切换）
  base_kp: 0.15
  base_kd: 0.5

  # 主动拦截控制更“凶”
  attack_kp: 0.35              # ↑
  attack_kd: 0.7               # ↑
  attack_lead_time: 3.0
  attack_bias: 0.7

  # 行为残差放大与PN导引增益
  base_alpha: 0.6
  residual_gain: 1.0
  pn_nav_gain: 3.0

  # 管理器周期（更快刷新分配）
  manager_period: 1            # 2 -> 1

  # 残差模仿触发预算下限（更易触发）
  imitation_min_budget: 0.02   # 0.10 -> 0.02

matcher:
  algo: hungarian              # 可选：greedy
  assign_lock_steps: 8         # 软锁定窗口
  switch_penalty: 12.0         # 切换成本，抑制频繁抖动

manager:
  mode: rule                   # rule | learned
  hidden: 256                  # 高层网络隐层宽度

manager_reward:
  cost_bonus_scale: 0.04       # 对比规则分配的平均截获时间改进
  coverage_bonus_scale: 0.2    # 鼓励额外覆盖攻击者
  threat_bonus_scale: 0.3      # 优先盯防威胁目标
  switch_penalty_scale: 0.05   # 避免频繁换防
  idle_penalty: 0.03           # 减少防守空转

manager_train:
  updates: 300
  horizon: 256
  gamma: 0.99
  gae_lambda: 0.95
  lr: 0.0003
  clip_eps: 0.2
  epochs: 4
  batch_size: 256
  value_coef: 0.5
  entropy_coef: 0.005
  teacher_coef: 0.4            # stronger imitation weight toward rule-based assignments
  teacher_mixing_start: 1.0    # probability of executing rule assignment at update 0
  teacher_mixing_end: 0.2      # anneal toward partial teacher forcing
  teacher_mixing_decay: 200    # updates over which to anneal mixing probability
  teacher_mixing_min: 0.35     # 不低于该概率，防止过早完全放弃示范
  max_grad_norm: 0.5
  target_kl: 0.1
  eval_every: 20
  eval_episodes: 40
  final_eval_episodes: 80
  controller_ckpt: ckpts/best.pt   # 先训练 controller，再加载权重训练 manager
  controller_deterministic: true   # manager 训练时使用确定性残差动作

train:
  updates: 400
  horizon: 600
  gamma: 0.99
  gae_lambda: 0.95
  lr: 0.0003
  clip_eps: 0.2
  epochs: 4
  batch_size: 1024
  value_coef: 0.5
  entropy_coef: 0.01

  # 残差模仿权重线性衰减
  imitation_w_start: 1.0
  imitation_w_end: 0.2
  imitation_decay_updates: 400
