seed: 0
device: auto
out_dir: runs/default

env:
  n_defenders: 5
  n_attackers: 5
  dt: 0.1
  max_steps: 600

  # 命中半径
  t_attack_radius: 10.0
  d_attack_radius: 18.0

  # 目标初速度（T）
  t_velocity: [10.0, 0.0, 0.0]

  # 攻击者(P)/防守者(D)的速度/加速度上限
  p_speed_max: 24.0
  p_accel_max: 6.0
  d_speed_max: 20.0
  d_accel_max: 4.0

  # 世界边界 [xmin,xmax], [ymin,ymax], [zmin,zmax]
  world_bounds: [[-1000.0, 1000.0], [-300.0, 300.0], [-150.0, 150.0]]

  # 开局生成参数
  spawn:
    t_x_frac: 0.5
    d_guard_radius: 40.0
    front_hit_time: 30.0
    lateral_spread_y: 400.0
    lateral_spread_z: 300.0
    min_pp_dist: 300.0
    min_tp_dist: 120.0
    spawn_attempts: 200

  # 奖励塑形（让 D 更积极接战）
  approach_reward_scale: 0.01   # ↑ 从 0.002 提升
  closing_reward_scale: 0.008   # 新增：按LOS闭合速度奖励
  kill_reward: 1.0              # ↑ 从 0.25 提升
  time_penalty: 0.0005          # ↓ 减小时间惩罚

control:
  # 基线控制策略（更偏向主动拦截）
  base_type: attack             # escort -> attack

  # 护航/基础控制的 PD（保留以便切换）
  base_kp: 0.15
  base_kd: 0.5

  # 主动拦截控制更“凶”
  attack_kp: 0.35              # ↑
  attack_kd: 0.7               # ↑
  attack_lead_time: 3.0
  attack_bias: 0.7

  # 行为残差放大与PN导引增益
  base_alpha: 0.6
  residual_gain: 1.0
  pn_nav_gain: 3.0

  # 管理器周期（更快刷新分配）
  manager_period: 1            # 2 -> 1

  # 残差模仿触发预算下限（更易触发）
  imitation_min_budget: 0.02   # 0.10 -> 0.02

matcher:
  algo: hungarian              # 可选：greedy
  assign_lock_steps: 8         # 软锁定窗口
  switch_penalty: 12.0         # 切换成本，抑制频繁抖动

train:
  updates: 400
  horizon: 600
  gamma: 0.99
  gae_lambda: 0.95
  lr: 0.0003
  clip_eps: 0.2
  epochs: 4
  batch_size: 1024
  value_coef: 0.5
  entropy_coef: 0.01

  # 残差模仿权重线性衰减
  imitation_w_start: 1.0
  imitation_w_end: 0.2
  imitation_decay_updates: 400
